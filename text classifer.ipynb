{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练集正负样本，用svm+tfidf建模，预测测试集，f1在86%左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('stop_words.txt','r',encoding='utf-8').readlines()\n",
    "stop_words = [word.strip() for word in stop_words]\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本99890个\n",
      "正样本25965个，占比0.259936\n",
      "负样本73925个，占比0.740064\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    查看训练集样本情况\n",
    "'''\n",
    "with open(\"./Total_Train_Data.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    positive_label = 0\n",
    "    negative_label = 0\n",
    "    total_label = 0 \n",
    "    for line in data:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            label = line.split(\"\\t\")[0]\n",
    "            if label == \"0\":\n",
    "                positive_label += 1\n",
    "            if label == \"1\":\n",
    "                negative_label += 1\n",
    "            total_label += 1\n",
    "print(\"总样本%d个\" % total_label)\n",
    "print(\"正样本%d个，占比%f\" % (positive_label, (positive_label / total_label)))\n",
    "print(\"负样本%d个，占比%f\" % (negative_label, (negative_label / total_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99891\n",
      "['']\n",
      "1803.5141744613647\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集并分词，耗时半小时左右；\n",
    "    训练集里有一行空格； \n",
    "'''\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = open('Total_Train_Data.txt').read()\n",
    "labels, texts, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels.append(content[0])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)\n",
    "        \n",
    "end = time.time()\n",
    "running_time = end-start\n",
    "\n",
    "print(running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    创建一个dataframe，列名为text和label\n",
    "'''\n",
    "\n",
    "import pandas\n",
    "from sklearn import preprocessing\n",
    "\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "train_x = trainDF['text']\n",
    "\n",
    "train_y = trainDF['label']\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99890,)\n",
      "(99890,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55619034\n",
      "1047925\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    训练集共分词55619034个， 非重复的分词有1047925个；\n",
    "'''\n",
    "lens = []\n",
    "sums = 0\n",
    "for i in range(len(trainDF['text'])):\n",
    "    data = trainDF['text'][i].split(\"/\")\n",
    "    sums += len(data)\n",
    "    lens.extend(data) \n",
    "print(sums)\n",
    "print(len(set(lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.加载测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    重新整理测试集的格式\n",
    "'''\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "test_data = open('TouTiao_testdata1_format.txt', 'r', encoding='utf-8')\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for line in test_data:\n",
    "    line = line.strip()\n",
    "\n",
    "    label = line.split(\"\\t\")[0]\n",
    "    test_labels.append(label)\n",
    "\n",
    "    text = line.split(\"\\t\")[1]\n",
    "    lists = []\n",
    "    for word in jieba.cut(text):\n",
    "        if word != \" \" and word not in stop_words:\n",
    "            lists.append(word)\n",
    "    test_texts.append(\"/\".join(lists))\n",
    "\n",
    "        \n",
    "#创建一个dataframe，列名为text和label\n",
    "testDF = pandas.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "testDF['label'] = test_labels\n",
    "test_x = testDF['text']\n",
    "\n",
    "test_y = testDF['label']\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199,)\n",
      "(199,)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SVM + tf-idf建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 测试百度的500条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：1000000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.64      0.72        53\n",
      "          1       0.88      0.95      0.91       146\n",
      "\n",
      "avg / total       0.86      0.86      0.86       199\n",
      "\n",
      "历时： 534.3831670284271\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    设置在全部corpus(训练集)中至少出现两次的词为一个特征；\n",
    "    考虑1-gram和2-gram；\n",
    "    设置使用的特征是相关性最大的前100万个特征；若不设置，默认使用现有的corpus中的500万多个特征，耗时久，而且准确度几乎没有提升；\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(min_df=2, max_df=1.0, \n",
    "                                   ngram_range=(1, 2), max_features=1000000) #  token_pattern='(?u)\\\\b\\\\w+\\\\b' 去掉单独的汉字\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "\n",
    "\n",
    "joblib.dump(tfidf_vect_ngram, 'tfidf_vect_ngram.pkl') \n",
    "\n",
    "\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "print(\"特征维度：%s\" % xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(xtrain_tfidf_ngram, train_y)\n",
    "\n",
    "joblib.dump(svm, 'svm.pkl')\n",
    "\n",
    "y_prediction = svm.predict(xtest_tfidf_ngram)\n",
    "\n",
    "print(classification_report(y_true=test_y, y_pred=y_prediction))\n",
    "\n",
    "end = time.time()\n",
    "print(\"历时：\", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 测试今日头条的199条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_feature_names()的长度： 1000000\n",
      "特征维度：1000000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.64      0.72        53\n",
      "          1       0.88      0.95      0.91       146\n",
      "\n",
      "avg / total       0.86      0.86      0.86       199\n",
      "\n",
      "历时： 361.1466934680939\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    设置在全部corpus(训练集)中至少出现两次的词为一个特征；\n",
    "    考虑1-gram和2-gram；\n",
    "    设置使用的特征是相关性最大的前100万个特征；若不设置，默认使用现有的corpus中的500万多个特征，耗时久，而且准确度几乎没有提升；\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(min_df=2, max_df=1.0, \n",
    "                                   ngram_range=(1, 2), max_features=1000000) #  token_pattern='(?u)\\\\b\\\\w+\\\\b' 去掉单独的汉字\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "\n",
    "print(\"get_feature_names()的长度：\", len(tfidf_vect_ngram.get_feature_names()))\n",
    "\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "print(\"特征维度：%s\" % xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "# xtrain_count, xtest_count = tfIdf(2, 1.0, (1, 2))\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(xtrain_tfidf_ngram, train_y)\n",
    "\n",
    "y_prediction = svm.predict(xtest_tfidf_ngram)\n",
    "\n",
    "print(classification_report(y_true=test_y, y_pred=y_prediction))\n",
    "\n",
    "end = time.time()\n",
    "print(\"历时：\", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v ul m n c d v r v', 'n n p nt n d v v v n', 'c d v r p eng v v p']\n",
      "{'v': 9, 'ul': 8, 'm': 3, 'n': 4, 'c': 0, 'd': 1, 'r': 7, 'p': 6, 'nt': 5, 'eng': 2}\n",
      "[[0.28297035 0.21975172 0.         0.37207201 0.28297035 0.\n",
      "  0.         0.28297035 0.37207201 0.65925516]\n",
      " [0.         0.15606936 0.         0.         0.80387084 0.26424839\n",
      "  0.20096771 0.         0.         0.46820807]\n",
      " [0.26958373 0.20935581 0.3544702  0.         0.         0.\n",
      "  0.53916745 0.26958373 0.         0.62806744]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    利用词性特征\n",
    "'''\n",
    "import jieba.posseg as pseg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tests = ['查了很多资料还是不知道怎么实现',\n",
    "'结巴分词跟中科院分词都能提取出词性',\n",
    "'可是不知道怎么跟sklearn结合起来用']\n",
    "\n",
    "def tag_trans(s):\n",
    "    words = pseg.cut(s)\n",
    "    return ' '.join([w.flag for w in words])\n",
    "\n",
    "tag_texts = [tag_trans(s) for s in tests]\n",
    "\n",
    "print(tag_texts)\n",
    "cvec = TfidfVectorizer(tokenizer=lambda x:str(x).split(' '))\n",
    "tag_vec = cvec.fit_transform(tag_texts).toarray()\n",
    "print(cvec.vocabulary_)\n",
    "print(tag_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
