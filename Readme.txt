任务2：
    做一个垃圾文本二分类模型
任务背景：
    公司小程序中展示的是一些科技类的信息，但数据库里的信息是从各地爬下来的，质量参差不齐，因此在展示时第一步需要先过滤一下垃圾信息，比如招聘信息，菜谱，广告等
解决方案：
    1.通过操作mysql和es，导出从36kr，头条，微信公众号，百度这些平台获得的近10万条据集，划分3/4负样本，1/4正样本。
    2. 划分正负样本，首先是人工标注，标注了大概500条样本，接着利用这些标注好的样本，利用svm模型预测新样本为正样本或负样本的可能，通过不断调整阈值，设置当正样本的可能性大于90%是为正样本，负样本的可能性大于90%时为负样本，将样本扩充到了10w条左右；
    3. 接着对样本进行分词，由于这里词本身就是特征，故采用分词效果不是最好但速度较快的jieba分词；分词后剔除停用词、特殊符号，用tf-idf提取一元/二元的词特征，设置最大特征数为5000；
    4. 接着用linear SVC建模；
测试结果：
    测试集准确率/召回率/f1得分均在80%左右。
改进措施：
    尝试用tf-idf做词性特征的提取，发现f1得分78%、用doc2vec做文本分类，f1得分36%，效果都不如第一次直接用tf-idf做词本身的特征提效果好。

--------------------------------------------------------------------------分割线----------------------------------------------------------------------------

当时敲得比较嗨，注释也没怎么写，现在啥都想不起来了，上面那一段话就是这个任务的全部内容。
1. 划分正负样本时，是用预测概率，卡了一个比较高的阈值，这部分代码，找不到在哪里了。。。
2. 后来尝试的词性提取，这部分代码，也找不到在哪里了。。。
3. 目录models里，就是这次任务的最终结果了，一个tfidf模型，一个smv模型。
3. 所以，以后写代码，一定及时写备注啊啊啊啊啊啊。。。