{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以忽略这一页，因为我也不知道写的是啥子了。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '我 爱', '爱']\n",
      "<bound method _cs_matrix.toarray of <3x3 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6 stored elements in Compressed Sparse Row format>>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2, ngram_range=(1, 2), token_pattern='(?u)\\\\b\\\\w+\\\\b') # 默认的token_pattern参数会过滤掉只有一个字符长度的词\n",
    "\n",
    "corpus = [\n",
    "    'I am a boy',\n",
    "    '我/爱/北京/天安门',\n",
    "    '我/爱/上海/外滩'\n",
    "    ]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print(feature_name)\n",
    "print(X.toarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 爱\n",
      "爱 北京\n",
      "北京 天安门\n",
      "天安门 也\n",
      "也 爱\n",
      "爱 上海\n",
      "上海 外滩\n",
      "我 爱 北京\n",
      "爱 北京 天安门\n",
      "北京 天安门 也\n",
      "天安门 也 爱\n",
      "也 爱 上海\n",
      "爱 上海 外滩\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 3), token_pattern=r'(?u)\\b\\w+\\b', min_df=2, max_df=1.0)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "for f in analyze('我 爱 北京 天安门 也 爱 上海 外滩'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 8, 'quick': 7, 'brown': 1, 'fox': 3, 'jumped': 4, 'over': 6, 'lazy': 5, 'dog': 2, 'belongs': 0, 'to': 9}\n",
      "(1, 10)\n",
      "[[1 1 2 2 1 1 1 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"The quick brown fox jumped over the lazy dog, the dog belongs to a fox.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.stop_words_)\n",
    "\n",
    "vector1 = vectorizer.transform(text)\n",
    "\n",
    "print(vector1.shape)\n",
    "\n",
    "print(vector1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 8)\t1\n",
      "[[0 0 1 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 编码其他文档\n",
    "text2 = [\"the happy fat dog\"]\n",
    "vector2 = vectorizer.transform(text2)\n",
    "print(vector2)\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print(feature_name)\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 文本文档列表\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "# 创建变换函数\n",
    "vectorizer = TfidfVectorizer()\n",
    "# 词条化以及创建词汇表\n",
    "vectorizer.fit(text)\n",
    "# 总结\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# 编码文档\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# 总结编码文档\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('stop_words.txt','r',encoding='utf-8').readlines()\n",
    "stop_words = [word.strip() for word in stop_words]\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4128\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import jieba\n",
    "\n",
    "\n",
    "## total_train_data.txt\n",
    "\n",
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('total_train_data.txt').read()\n",
    "labels, texts, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10318\n",
      "10318\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_2000_1.txt').read()\n",
    "labels1, texts1, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels1.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts1.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(labels1))\n",
    "print(len(texts1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_2000_2.txt').read()\n",
    "labels2, texts2, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels2.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts2.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(labels2))\n",
    "print(len(texts2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_2000_3.txt').read()\n",
    "labels3, texts3, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels3.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts3.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "print(len(labels3))\n",
    "print(len(texts3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_positive_sample_from_toutiao_2000.txt').read()\n",
    "labels4, texts4, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels4.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts4.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n",
      "604\n"
     ]
    }
   ],
   "source": [
    "print(len(labels4))\n",
    "print(len(texts4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4633\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_12000_1.txt').read()\n",
    "labels5, texts5, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels5.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts5.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4632\n",
      "4632\n"
     ]
    }
   ],
   "source": [
    "print(len(labels5))\n",
    "print(len(texts5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2912\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_positive_sample_from_toutiao_12000_1.txt').read()\n",
    "labels6, texts6, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels6.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts6.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2911\n",
      "2911\n"
     ]
    }
   ],
   "source": [
    "print(len(labels6))\n",
    "print(len(texts6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7349\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_22000_1.txt').read()\n",
    "labels7, texts7, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels7.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts7.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7348\n",
      "7348\n"
     ]
    }
   ],
   "source": [
    "print(len(labels7))\n",
    "print(len(texts7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_positive_sample_from_toutiao_22000_1.txt').read()\n",
    "labels8, texts8, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels8.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts8.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n",
      "889\n"
     ]
    }
   ],
   "source": [
    "print(len(labels8))\n",
    "print(len(texts8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18093\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_42000_1.txt').read()\n",
    "labels9, texts9, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels9.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts9.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18092\n",
      "18092\n"
     ]
    }
   ],
   "source": [
    "print(len(labels9))\n",
    "print(len(texts9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_positive_sample_from_toutiao_42000_1.txt').read()\n",
    "labels10, texts10, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels10.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts10.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n",
      "1198\n"
     ]
    }
   ],
   "source": [
    "print(len(labels10))\n",
    "print(len(texts10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37601\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_negative_sample_from_toutiao_92000_1.txt').read()\n",
    "labels11, texts11, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels11.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts11.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37600\n",
      "37600\n"
     ]
    }
   ],
   "source": [
    "print(len(labels11))\n",
    "print(len(texts11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6046\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "data = open('new_positive_sample_from_toutiao_92000_1.txt').read()\n",
    "labels12, texts12, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels12.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts12.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6045\n",
      "6045\n"
     ]
    }
   ],
   "source": [
    "print(len(labels12))\n",
    "print(len(texts12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels + labels12\n",
    "texts = texts + texts12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个dataframe，列名为text和label\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90030,)\n"
     ]
    }
   ],
   "source": [
    "print(trainDF['text'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集分为训练集和验证集\n",
    "# from sklearn import model_selection\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "train_x = trainDF['text']\n",
    "train_y = trainDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45408937\n",
      "972432\n"
     ]
    }
   ],
   "source": [
    "# 去掉一些停用词和空格之后，整个trainData共分成了673724个字/词，非重复词/字共54738个\n",
    "lens = []\n",
    "sums = 0\n",
    "for i in range(len(trainDF['text'])):\n",
    "    data = trainDF['text'][i].split(\"/\")\n",
    "    sums += len(data)\n",
    "    lens.extend(data) \n",
    "print(sums)\n",
    "print(len(set(lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10319\n",
      "99\n",
      "41\n",
      "253\n",
      "604\n",
      "4632\n",
      "2911\n",
      "7348\n",
      "889\n",
      "18092\n",
      "1198\n",
      "37600\n",
      "6045\n",
      "90031\n"
     ]
    }
   ],
   "source": [
    "data0 = open(\"../old/total_train_data.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "data1 = open(\"./new_negative_sample_from_toutiao_2000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data2 = open(\"./new_negative_sample_from_toutiao_2000_2.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data3 = open(\"./new_negative_sample_from_toutiao_2000_3.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data4 = open(\"./new_positive_sample_from_toutiao_2000.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "data5 = open(\"./new_negative_sample_from_toutiao_12000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data6 = open(\"./new_positive_sample_from_toutiao_12000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "data7 = open(\"./new_negative_sample_from_toutiao_22000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data8 = open(\"./new_positive_sample_from_toutiao_22000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "data9 = open(\"./new_negative_sample_from_toutiao_42000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data10 = open(\"./new_positive_sample_from_toutiao_42000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "data11 = open(\"./new_negative_sample_from_toutiao_92000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "data12 = open(\"./new_positive_sample_from_toutiao_92000_1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "print(len(data0))\n",
    "print(len(data1))\n",
    "print(len(data2))\n",
    "print(len(data3))\n",
    "print(len(data4))\n",
    "print(len(data5))\n",
    "print(len(data6))\n",
    "print(len(data7))\n",
    "print(len(data8))\n",
    "print(len(data9))\n",
    "print(len(data10))\n",
    "print(len(data11))\n",
    "print(len(data12))\n",
    "\n",
    "print(len(data0) + len(data1) + len(data2) + len(data3) + len(data4) + len(data5) + len(data6)\n",
    "     + len(data7) + len(data8) + len(data9) + len(data10) + len(data11) + len(data12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10318"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "385 + 528 + 3599 + 5195 + 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = open(\"./Total_train_data.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# for line in data0:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data1:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data2:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data3:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data4:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data5:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data6:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data7:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data8:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data9:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data10:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "\n",
    "# for line in data11:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")\n",
    "            \n",
    "# for line in data12:\n",
    "#     if line:\n",
    "#         line = line.strip()\n",
    "#         if line == \"\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             new_data.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90030\n"
     ]
    }
   ],
   "source": [
    "print(len(open(\"./Total_train_data.txt\", \"r\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本90030个\n",
      "正样本16105个，占比0.178885\n",
      "负样本73925个，占比0.821115\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Total_train_data.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    positive_label = 0\n",
    "    negative_label = 0\n",
    "    total_label = 0 \n",
    "    for line in data:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            label = line.split(\"\\t\")[0]\n",
    "            if label == \"0\":\n",
    "                positive_label += 1\n",
    "            if label == \"1\":\n",
    "                negative_label += 1\n",
    "            total_label += 1\n",
    "print(\"总样本%d个\" % total_label)\n",
    "print(\"正样本%d个，占比%f\" % (positive_label, (positive_label / total_label)))\n",
    "print(\"负样本%d个，占比%f\" % (negative_label, (negative_label / total_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = open(\"./Total_train_data.txt\", \"r\", encoding=\"utf-8\")\n",
    "data_2 = open(\"./Wechat_positive1_format.txt\", \"r\", encoding=\"utf-8\")\n",
    "new_data = open(\"./Total_Train_Data.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for line in data_1:\n",
    "    if line:\n",
    "        line = line.strip()\n",
    "        new_data.write(line + \"\\n\")\n",
    "for line in data_2:\n",
    "    if line:\n",
    "        line = line.strip()\n",
    "        new_data.write(line + \"\\n\")\n",
    "        \n",
    "data_1.close()\n",
    "data_2.close()\n",
    "new_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99890\n"
     ]
    }
   ],
   "source": [
    "print(len(open(\"./Total_Train_Data.txt\", \"r\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99891"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90030 + 9861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本99890个\n",
      "正样本25965个，占比0.259936\n",
      "负样本73925个，占比0.740064\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Total_Train_Data.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    positive_label = 0\n",
    "    negative_label = 0\n",
    "    total_label = 0 \n",
    "    for line in data:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            label = line.split(\"\\t\")[0]\n",
    "            if label == \"0\":\n",
    "                positive_label += 1\n",
    "            if label == \"1\":\n",
    "                negative_label += 1\n",
    "            total_label += 1\n",
    "print(\"总样本%d个\" % total_label)\n",
    "print(\"正样本%d个，占比%f\" % (positive_label, (positive_label / total_label)))\n",
    "print(\"负样本%d个，占比%f\" % (negative_label, (negative_label / total_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('../stop_words.txt','r',encoding='utf-8').readlines()\n",
    "stop_words = [word.strip() for word in stop_words]\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99891\n",
      "['']\n",
      "1803.5141744613647\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载训练数据集，并将文本分词\n",
    "'''\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = open('Total_Train_Data.txt').read()\n",
    "labels, texts, = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split(\"\\t\")\n",
    "    if len(content) == 2:\n",
    "        labels.append(content[0])\n",
    "    #     texts.append(content[1])\n",
    "        text = content[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        texts.append(\"/\".join(lists))\n",
    "    else:\n",
    "        print(i+1)\n",
    "        print(content)\n",
    "        \n",
    "end = time.time()\n",
    "running_time = end-start\n",
    "\n",
    "print(running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个dataframe，列名为text和label\n",
    "\n",
    "import pandas\n",
    "\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = trainDF['text']\n",
    "train_y = trainDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label编码为目标变量\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99890,)\n"
     ]
    }
   ],
   "source": [
    "print(trainDF['text'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55619034\n",
      "1047925\n"
     ]
    }
   ],
   "source": [
    "# 去掉一些停用词和空格之后，整个trainData共分成了673724个字/词，非重复词/字共54738个\n",
    "lens = []\n",
    "sums = 0\n",
    "for i in range(len(trainDF['text'])):\n",
    "    data = trainDF['text'][i].split(\"/\")\n",
    "    sums += len(data)\n",
    "    lens.extend(data) \n",
    "print(sums)\n",
    "print(len(set(lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JinRiTouTiao/46557.txt\n",
      "./JinRiTouTiao/50232.txt\n",
      "./JinRiTouTiao/51407.txt\n",
      "./JinRiTouTiao/51862.txt\n",
      "./JinRiTouTiao/51947.txt\n",
      "./JinRiTouTiao/54234.txt\n",
      "./JinRiTouTiao/55921.txt\n",
      "./JinRiTouTiao/55978.txt\n",
      "./JinRiTouTiao/57447.txt\n",
      "./JinRiTouTiao/59822.txt\n",
      "./JinRiTouTiao/61256.txt\n",
      "./JinRiTouTiao/68569.txt\n",
      "./JinRiTouTiao/70948.txt\n",
      "./JinRiTouTiao/81157.txt\n",
      "./JinRiTouTiao/82488.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    加载头条全部数据的前一千条，共1千条，仅有context没有label，后面用阈值预测，实际只取出来998条\n",
    "    加载头条全部数据的前1千到2千条，共1千条，仅有context没有label，后面用阈值预测，实际只取出来987条\n",
    "    加载头条全部数据的前2千到12千条，共1万条，仅有context没有label，后面用阈值预测，实际只取出来9982条, 取了2911条正样本，4632条负样本，其余的数据扔了\n",
    "    加载头条全部数据的前12千到22千条，共1万条，仅有context没有label，后面用阈值预测，实际只取出来9988条, 取了889条正样本，7348条负样本，其余的数据扔了\n",
    "    加载头条全部数据的前22千到42千条，共2万条，仅有context没有label，后面用阈值预测，实际只取出来19988条, 取了1198条正样本，18098条负样本，其余的数据扔了\n",
    "    加载头条全部数据的前42千到92054条，共5万多条，仅有context没有label，后面用阈值预测，实际只取出来50078条, 取了 条正样本，37600条负样本，其余的数据扔了\n",
    "'''\n",
    "inputs = open(\"./JinRiTouTiao_format_92000.txt\", \"w\", encoding=\"utf-8\")\n",
    "import os\n",
    "for file in os.listdir(\"./JinRiTouTiao\"):\n",
    "    if file.split(\".\")[0].isdigit():\n",
    "        if 42001 <= int(file.split(\".\")[0]) <= 92094:\n",
    "            f = open(\"./JinRiTouTiao/%s\" % file, \"r\", encoding=\"utf-8\")\n",
    "            data = f.readlines()\n",
    "            if len(data) != 2:\n",
    "                print(\"./JinRiTouTiao/%s\" % file)\n",
    "                continue\n",
    "            else:\n",
    "                sents = []\n",
    "                for sent in data:\n",
    "                    sent = sent.strip()\n",
    "                    sents.append(sent)\n",
    "                inputs.write(\"。\".join(sents) + \"\\n\")\n",
    "inputs.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12478\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    整理剔除已标label的数据，剩下的数据作为新的测试集\n",
    "'''\n",
    "new_test_data = open('JinRiTouTiao_format_92000_drop1.txt', 'w', encoding='utf-8')\n",
    "test_data = open(\"./JinRiTouTiao_format_92000.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "# print(len(data))\n",
    "count = 0\n",
    "for i in range(len(test_data)):\n",
    "    if i+1  not in negative_samples1:\n",
    "        count += 1\n",
    "#         print(data[i])\n",
    "#         print(\"************************************\")\n",
    "        new_test_data .write(test_data[i]) \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37600\n"
     ]
    }
   ],
   "source": [
    "print(len(negative_samples1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50078"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12478 + 37600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    重新整理测试集的格式\n",
    "'''\n",
    "\n",
    "test_data = open('Baidu_TestData_1_format.txt', 'r', encoding='utf-8')\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "i = 1\n",
    "for line in test_data:\n",
    "    line = line.strip()\n",
    "    \n",
    "    if len(line.split(\"\\t\"))== 2:\n",
    "\n",
    "        label = line.split(\"\\t\")[0]\n",
    "        test_labels.append(label)\n",
    "\n",
    "        text = line.split(\"\\t\")[1]\n",
    "        lists = []\n",
    "        for word in jieba.cut(text):\n",
    "            if word != \" \" and word not in stop_words:\n",
    "                lists.append(word)\n",
    "        test_texts.append(\"/\".join(lists))\n",
    "        i += 1\n",
    "\n",
    "    else:\n",
    "        print(i)\n",
    "        \n",
    "#创建一个dataframe，列名为text和label\n",
    "testDF = pandas.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "testDF['label'] = test_labels\n",
    "test_x = testDF['text']\n",
    "test_y = testDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label编码为目标变量\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
      " 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
      " 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1\n",
      " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
      " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    计数向量作为特征\n",
    "'''\n",
    "def countVector(min_df, max_df, ngram_range):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    #创建一个向量计数器对象\n",
    "    count_vect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', min_df=min_df, max_df=max_df, ngram_range=ngram_range) # min_df设置为1，提取出来53714个特征词，min_df设置为2，提取出来22134个特征词\n",
    "    count_vect.fit(trainDF['text'])\n",
    "\n",
    "    #使用向量计数器对象转换训练集和验证集\n",
    "    xtrain_count = count_vect.transform(train_x)\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    \n",
    "    print(\"特征维度：%s\" % xtrain_count.shape[1])\n",
    "    \n",
    "    return xtrain_count, xtest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  tf-idf为特征  \n",
    "'''\n",
    "def tfIdf(min_df, max_df, ngram_range):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf_vect_ngram = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', min_df=min_df, max_df=max_df, ngram_range=ngram_range) #  ngram_range=(2,3),\n",
    "    tfidf_vect_ngram.fit(trainDF['text'])\n",
    "    xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "   \n",
    "    print(\"特征维度：%s\" % xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "    return xtrain_tfidf_ngram, xtest_tfidf_ngram  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestParameter(model, vect, train_x, train_y):\n",
    "\n",
    "    from time import time\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    categories = [\n",
    "        '0',\n",
    "        '1',\n",
    "    ]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect),\n",
    "    #     ('tfidf', TfidfTransformer()),\n",
    "        ('lr', model),\n",
    "    ])\n",
    "\n",
    "\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect__min_df':(1, 2, 3, 4),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2), (2, 3)),  # unigrams or bigrams\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 905.439s\n",
      "Best score: 0.933\n",
      "Best parameters set:\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bestParameter(LinearSVC(), CountVectorizer(), train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    词嵌入作为特征\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    词嵌入作为特征\n",
    "'''\n",
    "\n",
    "# #加载预先训练好的词嵌入向量\n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('word2vec.txt', \"r\", encoding=\"utf-8\")):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# #创建一个分词器\n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(trainDF['text'])\n",
    "# word_index = token.word_index\n",
    "\n",
    "# #将文本转换为分词序列，并填充它们保证得到相同长度的向量\n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# #创建分词嵌入映射\n",
    "# embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "# if embedding_vector is not None:\n",
    "#     embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_cnn():\n",
    "#     # Add an Input Layer\n",
    "#     input_layer = layers.Input((70, ))\n",
    "\n",
    "#     # Add the word embedding Layer\n",
    "#     embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "#     embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "#     # Add the convolutional Layer\n",
    "#     conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "#     # Add the pooling Layer\n",
    "#     pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "#     # Add the output Layers\n",
    "#     output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "#     output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "#     output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "#     model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# classifier = create_cnn()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "\n",
    "# print(\"CNN, Word Embeddings\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证词频特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_test(classifier, feature_vector_train, label, feature_vector_test, is_neural_net=False):\n",
    "    \n",
    "#     from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "    \n",
    "    lable_total = 0\n",
    "    label_positive = 0\n",
    "    label_negative =0\n",
    "    \n",
    "    for label in predictions:\n",
    "        if label == 0:\n",
    "            label_positive += 1\n",
    "            lable_total += 1\n",
    "        if label ==1:\n",
    "            label_negative += 1\n",
    "            lable_total += 1\n",
    "    print(\"预测结果中，正样本占比%f\" % (label_positive / lable_total))\n",
    "    print(\"预测结果中，负样本占比%f\" % (label_negative / lable_total))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_valid(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "# #     from sklearn.metrics import classification_report\n",
    "#     from sklearn import metrics\n",
    "#     from sklearn.metrics import brier_score_loss\n",
    "\n",
    "#     classifier.fit(feature_vector_train, label)\n",
    "#     predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "# #     predict_proba = classifier.predict_proba(feature_vector_valid)\n",
    "    \n",
    "# #     predict_proba_list = []\n",
    "    \n",
    "# #     for proba_pair in predict_proba:\n",
    "# #         if proba_pair[0] > 0.55:\n",
    "# #             predict_proba_list.append(0)\n",
    "# #         else:\n",
    "# #             predict_proba_list.append(1)\n",
    "\n",
    "# # #     predict_proba_list = \" \".join(predict_proba_list)\n",
    "    \n",
    "#     if is_neural_net:\n",
    "#         predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "#     print(\"pred label:\",predictions)\n",
    "    \n",
    "# #     print(classification_report(y_true=valid_y, y_pred=predictions))\n",
    "# #     print(brier_score_loss(valid_y, predictions))\n",
    "    \n",
    "#     return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证tf-idf特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_test(classifier, feature_vector_train, label, feature_vector_test, is_neural_net=False):\n",
    "#     # fit the training dataset on the classifier\n",
    "#     classifier.fit(feature_vector_train, label)\n",
    "\n",
    "#     # predict the labels on validation dataset\n",
    "#     predictions = classifier.predict(feature_vector_test)\n",
    "# #     predict_proba = classifier.predict_proba(feature_vector_test)\n",
    "    \n",
    "# #     predict_proba_list = []\n",
    "    \n",
    "# #     for proba_pair in predict_proba:\n",
    "# #         if proba_pair[0] > 0.55:\n",
    "# #             predict_proba_list.append(0)\n",
    "# #         else:\n",
    "# #             predict_proba_list.append(1)\n",
    "            \n",
    "# #     predict_proba_list = \" \".join(predict_proba_list)\n",
    "    \n",
    "#     if is_neural_net:\n",
    "#         predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "#     print(\"pred label:\", predictions)\n",
    "# #     print(\"predict_proba_list:\", predict_proba_list)\n",
    "    \n",
    "#     print(classification_report(y_true=test_y, y_pred=predictions))\n",
    "# #     print(classification_report(y_true=test_y, y_pred=predict_proba_list))\n",
    "    \n",
    "#     return metrics.accuracy_score(predictions, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Predict result by predict==\n",
      "[1]\n",
      "==Predict result by predict_proba==\n",
      "[[9.99999949e-01 5.05653254e-08]]\n",
      "==Predict result by predict_log_proba==\n",
      "[[-5.05653266e-08 -1.67999998e+01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "#拟合数据\n",
    "clf.fit(X, Y)\n",
    "print(\"==Predict result by predict==\")\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(\"==Predict result by predict_proba==\")\n",
    "print(clf.predict_proba([[-0.8, -1]]))\n",
    "print(\"==Predict result by predict_log_proba==\")\n",
    "print(clf.predict_log_proba([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：372589\n",
      "pred label: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "predict_proba_list [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      1.00      0.63        54\n",
      "          1       1.00      0.19      0.32        78\n",
      "\n",
      "avg / total       0.78      0.52      0.45       132\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       117\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "NB, Count Vectors:  0.5227272727272727\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的朴素贝叶斯\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "xtrain_count, xvalid_count = countVector(1, 0.7, (1, 2))\n",
    "accuracy = train_model_valid(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"NB, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：15920\n",
      "pred label: [1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "predict_proba_list [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.87      0.70        54\n",
      "          1       0.87      0.58      0.69        78\n",
      "\n",
      "avg / total       0.75      0.70      0.70       132\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        80\n",
      "          1       0.80      1.00      0.89        52\n",
      "\n",
      "avg / total       0.92      0.90      0.90       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "NB, N-Gram Vectors:  0.696969696969697\n"
     ]
    }
   ],
   "source": [
    "#特征为多个词语级别TF-IDF向量的朴素贝叶斯\n",
    "\n",
    "xtrain_count, xvalid_count = tfIdf(3, 0.5, (1, 1))\n",
    "accuracy = train_model_valid(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：372571\n",
      "pred label: [1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.67      0.70        54\n",
      "          1       0.78      0.83      0.81        78\n",
      "\n",
      "avg / total       0.76      0.77      0.76       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "LR, Count Vectors:  0.7651515151515151\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的逻辑回归\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "xtrain_count, xvalid_count = countVector(1, 0.5, (1, 2))\n",
    "accuracy = train_model_valid(LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"LR, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：46711\n",
      "pred label: [1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
      " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.59      0.66        54\n",
      "          1       0.75      0.86      0.80        78\n",
      "\n",
      "avg / total       0.75      0.75      0.74       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "LR, N-Gram Vectors:  0.75\n"
     ]
    }
   ],
   "source": [
    "#特征为多个词语级别TF-IDF向量的逻辑回归\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "xtrain_count, xvalid_count = tfIdf(3, 0.75, (1, 2))\n",
    "accuracy = train_model_valid(LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：22960286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.71      0.77       250\n",
      "          1       0.75      0.86      0.80       250\n",
      "\n",
      "avg / total       0.79      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "start = time.time()\n",
    "xtrain_count, xtest_count = countVector(1, 0.75, (1, 2)) # 1, 1.0 (1,1) ; 1, 0.75, (1,2)\n",
    "# train_model_test(LinearSVC(), xtrain_count, train_y, xtest_count)\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "# clf = CalibratedClassifierCV(svm) \n",
    "\n",
    "# clf.fit(xtrain_count, train_y)\n",
    "\n",
    "svm.fit(xtrain_count, train_y)\n",
    "\n",
    "y_prediction = svm.predict(xtest_count)\n",
    "\n",
    "print(classification_report(y_true=test_y, y_pred=y_prediction))\n",
    "\n",
    "end = time.time()\n",
    "print(\"用时：\", (end- start))\n",
    "\n",
    "# y_proba = clf.predict_proba(xtest_count)\n",
    "\n",
    "# lists = []\n",
    "# for i,j in y_proba:  \n",
    "#     if i > 0.9:  \n",
    "#         lists.append(0)\n",
    "#     else:\n",
    "#         lists.append(1)\n",
    "# print(lists)\n",
    "\n",
    "\n",
    "# lable_total = 0\n",
    "# label_positive = 0\n",
    "# label_negative =0\n",
    "# negative_list = []\n",
    "\n",
    "# for i, label in enumerate(lists):\n",
    "#     if label == 0:\n",
    "#         label_positive += 1\n",
    "#         lable_total += 1\n",
    "#     if label ==1:\n",
    "#         negative_list.append(i+1)\n",
    "#         label_negative += 1\n",
    "#         lable_total += 1\n",
    "# print(\"预测的负样本为\", negative_list)\n",
    "# print(\"预测的负样本个数\", len(negative_list))\n",
    "# print(\"预测结果中，正样本占比%f\" % (label_positive / lable_total))\n",
    "# print(\"预测结果中，负样本占比%f\" % (label_negative / lable_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_negative_samples = open(\"./new_negative_sample_from_toutiao_2000_3.txt\", \"w\", encoding=\"utf-8\")\n",
    "data = open(\"./JinRiTouTiao_format_2000.txt\", \"r\", encoding=\"utf-8\")\n",
    "i = 1\n",
    "for line in data:\n",
    "    line = line.strip()\n",
    "    if i in negative_samples2:\n",
    "        new_negative_samples.write(\"1\" + \"\\t\" + line + \"\\n\")\n",
    "        i += 1\n",
    "    if i not in negative_samples3:\n",
    "        i += 1\n",
    "        continue\n",
    "new_negative_samples.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试百度的500条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_feature_names()的长度： 1000000\n",
      "特征维度：1000000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.74      0.77       250\n",
      "          1       0.76      0.82      0.79       250\n",
      "\n",
      "avg / total       0.78      0.78      0.78       500\n",
      "\n",
      "历时： 365.1423580646515\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(min_df=2, max_df=1.0, \n",
    "                                   ngram_range=(1, 2), max_features=1000000) #  token_pattern='(?u)\\\\b\\\\w+\\\\b' 去掉单独的汉字\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "\n",
    "print(\"get_feature_names()的长度：\", len(tfidf_vect_ngram.get_feature_names()))\n",
    "\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "print(\"特征维度：%s\" % xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(xtrain_tfidf_ngram, train_y)\n",
    "\n",
    "y_prediction = svm.predict(xtest_tfidf_ngram)\n",
    "\n",
    "print(classification_report(y_true=test_y, y_pred=y_prediction))\n",
    "\n",
    "end = time.time()\n",
    "print(\"历时：\", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
      " 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
      " 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1\n",
      " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
      " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1\n",
      " 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0\n",
      " 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
      " 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
      " 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
      " 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
      " 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1\n",
      " 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
      " 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "[3, 5, 7, 13, 14, 15, 18, 20, 22, 30, 36, 37, 38, 42, 46, 52, 53, 65, 83, 86, 87, 88, 92, 95, 97, 98, 102, 104, 108, 110, 114, 116, 119, 121, 122, 124, 138, 156, 158, 164, 174, 194, 196, 216, 218, 223, 235, 237, 238, 241, 242, 248, 250, 251, 252, 254, 263, 265, 267, 269, 271, 272, 275, 282, 284, 285, 286, 296, 297, 303, 308, 314, 315, 318, 320, 322, 325, 339, 340, 344, 345, 349, 350, 355, 356, 364, 369, 373, 380, 387, 388, 389, 394, 398, 403, 404, 417, 424, 430, 431, 435, 437, 453, 460, 465, 475, 482, 486, 499]\n"
     ]
    }
   ],
   "source": [
    "wrong_label_list = []\n",
    "for i, label_pair in enumerate(zip(test_y, y_prediction)):\n",
    "        if label_pair[0] != label_pair[1]:\n",
    "            wrong_label_list.append(i+1)\n",
    "print(len(wrong_label_list))\n",
    "print(wrong_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3,5,13,14,30,42,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 没限制max_features时输出的特征，5518764个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 两重', '2 两项', '2 严守', '2 严格', '2 严格执行', '2 严禁', '2 严重', '2 丨', '2 个人', '2 个位', '2 个体', '2 个别', '2 个团', '2 个大类', '2 个性', '2 个性化', '2 个村', '2 个柏忌', '2 个点', '2 个班', '2 个球', '2 个税', '2 个端', '2 个股', '2 个色', '2 个队', '2 中', '2 中井柏然', '2 中介', '2 中信', '2 中信证券', '2 中兴通讯', '2 中医', '2 中午', '2 中华', '2 中华人民共和国', '2 中原', '2 中国', '2 中国人民大学', '2 中国建设银行', '2 中国政府', '2 中国电信', '2 中国移动', '2 中国联通', '2 中国证监会', '2 中国银行', '2 中外', '2 中央', '2 中学', '2 中对战', '2 中小', '2 中小企业', '2 中小学', '2 中层', '2 中山市', '2 中式', '2 中心', '2 中心化', '2 中报', '2 中文', '2 中期', '2 中枢', '2 中标', '2 中潜', '2 中火', '2 中级', '2 中纸', '2 中线', '2 中美', '2 中考', '2 中英文', '2 中药', '2 中药饮片', '2 中证报', '2 中部', '2 中银', '2 中长期', '2 中间', '2 中风', '2 中餐厅', '2 丰厚', '2 丰富', '2 丰富多彩', '2 串', '2 临床', '2 临床试验', '2 临时', '2 丸', '2 丹麦', '2 为例', '2 为求', '2 主', '2 主体', '2 主创', '2 主力', '2 主动', '2 主卧', '2 主场', '2 主导产业', '2 主干']\n"
     ]
    }
   ],
   "source": [
    "# 没限制max_features时输出的特征\n",
    "print(tfidf_vect_ngram.get_feature_names()[42000:42100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 限制max_features为2500000时的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['家 央视', '家 头部', '家 奖励', '家 奖金', '家 女人', '家 女儿', '家 奶奶', '家 好', '家 好好', '家 如今', '家 妈妈', '家 妻子', '家 始终', '家 姐姐', '家 婆婆', '家 媒体', '家 媒体报道', '家 媳妇', '家 子公司', '家 字', '家 存在', '家 学', '家 学习', '家 学校', '家 学霸', '家 孩子', '家 安', '家 安全', '家 安徽', '家 安心', '家 安装', '家 安防', '家 完成', '家 官方', '家 定做', '家 定制', '家 定点', '家 宜春市', '家 宝宝', '家 宝贝', '家 实体', '家 实体店', '家 实力', '家 实现', '家 实际', '家 宠物', '家 客', '家 客厅', '家 客户', '家 宣布', '家 家', '家 家中', '家 家人', '家 家居', '家 家庭', '家 家政', '家 家装', '家 家里', '家 家门口', '家 富贵', '家 对冲', '家 对比', '家 对面', '家 导致', '家 寿险', '家 封板', '家 小', '家 小兔', '家 小区', '家 小孩', '家 小孩子', '家 小店', '家 小朋友', '家 小米', '家 小额贷款', '家 小龙', '家 少爷', '家 尝试', '家 尤其', '家 就要', '家 居', '家 居住', '家 居然', '家 屈臣氏', '家 屏幕', '家 展出', '家 展台', '家 展商', '家 属于', '家 山', '家 山东', '家 山东省', '家 山西', '家 山西省', '家 山鹰', '家 工业', '家 工作', '家 工作人员', '家 工厂', '家 工地']\n"
     ]
    }
   ],
   "source": [
    "# 限制max_features为2500000时的特征\n",
    "print(tfidf_vect_ngram.get_feature_names()[1000000:1000100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0 0', '0 00', '0 000', '0 0000', '0 000000', '0 0001', '0 0005', '0 0007', '0 0009', '0 001', '0 0018', '0 002', '0 0025', '0 003', '0 004', '0 005', '0 006', '0 007', '0 008', '0 01', '0 012', '0 014', '0 015', '0 016', '0 0192', '0 02', '0 023', '0 025', '0 028', '0 03', '0 032', '0 033', '0 034', '0 035', '0 036', '0 037', '0 04', '0 040', '0 041', '0 044', '0 045', '0 05', '0 051', '0 053', '0 055', '0 058', '0 06', '0 062', '0 064', '0 065', '0 067', '0 068', '0 07', '0 070', '0 072', '0 073', '0 074', '0 075', '0 0758', '0 076', '0 08', '0 081', '0 084', '0 085', '0 087', '0 09', '0 090', '0 093', '0 096', '0 098', '0 099', '0 1', '0 10', '0 100', '0 1000', '0 100000', '0 100km', '0 102', '0 103', '0 104', '0 104cfu', '0 105', '0 107', '0 109', '0 11', '0 110', '0 111', '0 114', '0 115', '0 119', '0 12', '0 120', '0 123', '0 124', '0 125', '0 126', '0 127', '0 13', '0 130']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['７ ｍ', '８', '８ 岁', '８ 平', '８ 年', '８ 日', '８ 日电', '８ 月', '８ 进', '８ ０', '８ １', '８ ２', '８ ３', '８ ４', '８ ５', '８ ６', '８ ７', '８ ９', '８ ｍ', '９', '９ 平', '９ 年', '９ 日', '９ 日电', '９ 月', '９ ０', '９ １', '９ ２', '９ ３', '９ ４', '９ ５', '９ ６', '９ ７', '９ ８', '９ ９', 'ａ', 'ｂ', 'ｂ 级', 'ｂ ｔ', 'ｃ', 'ｃ 级', 'ｃ ｅ', 'ｃ ｍ', 'ｃ ｏ', 'ｃ ｓ', 'ｄ', 'ｄ ｐ', 'ｅ', 'ｅ 孔院', 'ｅ ｄ', 'ｅ ｏ', 'ｅ ｒ', 'ｆ', 'ｆ ｉ', 'ｇ', 'ｇ ｄ', 'ｇ ｌ', 'ｈ', 'ｈ ｍ', 'ｉ', 'ｉ ｃ', 'ｉ ｄ', 'ｉ ｏ', 'ｉ ｐ', 'ｉ ｔ', 'ｋ', 'ｋ 线', 'ｋ ｇ', 'ｌ', 'ｌ ｅ', 'ｍ', 'ｍ 定植', 'ｍ 时', 'ｍ ２', 'ｍ ｇ', 'ｍ ｍ', 'ｍ ｓ', 'ｎ', 'ｎ ｙ', 'ｏ', 'ｏ ｅ', 'ｏ ｓ', 'ｐ', 'ｐ ｐ', 'ｑ', 'ｑ ｑ', 'ｒ', 'ｓ', 'ｓ ｃ', 'ｓ ｔ', 'ｔ', 'ｔ ３', 'ｔ ｃ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｙ ｔ']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[-100:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "fencoding=chardet.detect(b'\"\".join(tfidf_vect_ngram.get_feature_names()[0:100]')\n",
    "print(fencoding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes {'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n",
      "str1 {'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n",
      "str2 {'encoding': 'GB2312', 'confidence': 0.99, 'language': 'Chinese'}\n",
      "str3 {'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n",
      "str4 {'encoding': 'utf-8', 'confidence': 0.9690625, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet as chardet\n",
    "\n",
    "def check_char(content):\n",
    "    return chardet.detect(content)\n",
    "\n",
    "# 默认只接受byte_str，否则返回TypeError\n",
    "print(\"bytes\", check_char(b\"hello\")) \n",
    "print(\"str1\", check_char(\"hello world\".encode(\"gbk\"))) \n",
    "print(\"str2\", check_char(\"老子回来啦\".encode(\"gbk\")))\n",
    "print(\"str3\", check_char(\"hello world\".encode(\"utf-8\"))) \n",
    "print(\"str4\", check_char(\"老子回来啦\".encode(\"utf-8\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 限制max_features=1000000时输出的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0 0', '0 00', '0 000', '0 0000', '0 000000', '0 0001', '0 0005', '0 001', '0 002', '0 003', '0 004', '0 005', '0 006', '0 007', '0 01', '0 012', '0 016', '0 02', '0 025', '0 03', '0 04', '0 05', '0 058', '0 06', '0 065', '0 067', '0 07', '0 08', '0 087', '0 09', '0 096', '0 098', '0 1', '0 10', '0 100', '0 1000', '0 100km', '0 11', '0 110', '0 12', '0 120', '0 125', '0 13', '0 131', '0 14', '0 15', '0 16', '0 17', '0 18', '0 180', '0 19', '0 192', '0 2', '0 20', '0 200', '0 2015', '0 2017', '0 2018', '0 21', '0 22', '0 23', '0 24', '0 25', '0 255', '0 26', '0 27', '0 275', '0 28', '0 280', '0 29', '0 3', '0 30', '0 31', '0 310', '0 315', '0 32', '0 320', '0 33', '0 34', '0 35', '0 36', '0 37', '0 38', '0 380', '0 39', '0 4', '0 40', '0 400', '0 41', '0 42', '0 43', '0 44', '0 45', '0 46', '0 47', '0 48', '0 49', '0 5', '0 50']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['５ 平', '５ 年', '５ 日', '５ 月', '５ 进', '５ 退', '５ ０', '５ １', '５ ２', '５ ３', '５ ４', '５ ５', '５ ６', '５ ７', '５ ８', '５ ｄ', '６', '６ 平', '６ 年', '６ 月', '６ 进', '６ 退', '６ ０', '６ １', '６ ２', '６ ４', '６ ５', '６ ６', '６ ７', '７', '７ 平', '７ 年', '７ 日', '７ 月', '７ 进', '７ ０', '７ １', '７ ２', '７ ３', '７ ５', '７ ６', '７ ７', '７ ８', '７ ９', '８', '８ 年', '８ 日', '８ 月', '８ ０', '８ １', '８ ｍ', '９', '９ 年', '９ 月', '９ ０', '９ １', '９ ２', '９ ３', '９ ４', '９ ５', '９ ６', '９ ７', '９ ８', '９ ９', 'ａ', 'ｂ', 'ｃ', 'ｃ ｍ', 'ｄ', 'ｄ ｐ', 'ｅ', 'ｅ ｄ', 'ｅ ｒ', 'ｆ', 'ｇ', 'ｇ ｄ', 'ｈ', 'ｉ', 'ｋ', 'ｋ 线', 'ｋ ｇ', 'ｌ', 'ｍ', 'ｍ ２', 'ｍ ｍ', 'ｎ', 'ｎ ｙ', 'ｏ', 'ｐ', 'ｐ ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｘ', 'ｙ', 'ｙ ｔ']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[-100:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['轴 y', '轴 代表', '轴 位置', '轴 方向', '轴 旋转', '轴 曲面', '轴 行程', '轴 表示', '轴 都', '轴上', '轴向', '轴对称', '轴心', '轴心 时代', '轴心国', '轴承', '轴承 公司', '轴承 磨损', '轴瓦', '轴突', '轴线', '轴线 上', '轴距', '轴距 2640mm', '轴距 增加', '轴距 达到', '轵', '轵 关陉', '轶', '轶事', '轶闻', '轸', '轻', '轻 300', '轻 一声', '轻 一点', '轻 不', '轻 办公', '轻 嗅', '轻 备注', '轻 小说', '轻 山东地区', '轻 幽默', '轻 应用', '轻 快', '轻 捻', '轻 智能', '轻 更', '轻 服务', '轻 模式', '轻 没有', '轻 游戏', '轻 生活', '轻 社交', '轻 薄', '轻 装修', '轻 谢谢', '轻 资产', '轻 轻', '轻 轻放', '轻 运营', '轻 重', '轻 钱包', '轻 问诊', '轻 餐饮', '轻中度', '轻举妄动', '轻仓', '轻伤', '轻伤 不下', '轻伤 二级', '轻似', '轻体', '轻佻', '轻便', '轻便 凉爽', '轻信', '轻元', '轻元 科技', '轻则', '轻功', '轻医美', '轻卡', '轻取', '轻叹', '轻叹 一声', '轻吟', '轻启', '轻吹', '轻吻', '轻响', '轻唤', '轻唱', '轻喜剧', '轻型', '轻型 柴油车', '轻型 纸', '轻型 飞机', '轻型化', '轻声']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[900000:900100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 限制max_features=1000000时输出的特征，去掉单独的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00 00', '00 01', '00 02', '00 03', '00 04', '00 05', '00 06', '00 07', '00 08', '00 09', '00 10', '00 11', '00 12', '00 13', '00 14', '00 15', '00 16', '00 17', '00 18', '00 19', '00 20', '00 2017', '00 2018', '00 21', '00 22', '00 23', '00 24', '00 25', '00 26', '00 27', '00 28', '00 2800', '00 29', '00 30', '00 35', '00 36', '00 40', '00 50', '00 60', '00 90', '00 am', '00 ceo', '00 paddlepaddle', '00 pm', '00 万元', '00 万股', '00 万至', '00 三层', '00 上海', '00 上班', '00 上线', '00 下午', '00 下单', '00 世界杯', '00 中国', '00 为主', '00 主题', '00 之前', '00 之后', '00 之间', '00 乘车', '00 亿元', '00 代言人', '00 以上', '00 以后', '00 价格', '00 会议', '00 会议室', '00 俄罗斯', '00 停止', '00 停电', '00 八年', '00 公司', '00 决赛', '00 准时', '00 创业者', '00 初六', '00 到达', '00 北京', '00 原有', '00 发行', '00 台风', '00 同比', '00 后们', '00 含税', '00 周五', '00 周六', '00 周末', '00 咨询电话', '00 哥伦比亚', '00 嘉宾', '00 四年', '00 地址', '00 地点', '00 孩子', '00 完成', '00 实时', '00 实际', '00 客商']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['龙街', '龙袍', '龙袍 男子', '龙要', '龙要 纸板厂', '龙观', '龙角', '龙说', '龙越', '龙越 慈善', '龙跃', '龙身', '龙车', '龙辉', '龙达', '龙达 纸业', '龙里', '龙钢', '龙铁', '龙铁 纵横', '龙镇', '龙镇 鹏翔', '龙门', '龙门 创将', '龙门 教育', '龙门 飞甲', '龙门县', '龙门吊', '龙门客栈', '龙门山', '龙门石窟', '龙门阵', '龙阳', '龙阳 大道', '龙陵', '龙隐镇', '龙霸', '龙非夜', '龙韵', '龙韵 股份', '龙须', '龙须沟', '龙须草', '龙须面', '龙颜', '龙颜大悦', '龙飞', '龙飞 mba', '龙飞凤舞', '龙首', '龙马', '龙马 环卫', '龙驹', '龙骑士', '龙骨', '龙魂', '龙鱼', '龙龙', '龙龟', '龚佳', '龚先生', '龚克', '龚宇', '龚宇 爱奇艺', '龚宇 表示', '龚家', '龚小京', '龚强', '龚怡宏', '龚成', '龚昕', '龚某', '龚滩', '龚焱', '龚焱 中欧国际工商学院', '龚焱 区块', '龚老', '龚菲菲', '龚虹嘉', '龚贤', '龚进辉', '龚钦', '龚钦 表示', '龟儿子', '龟兔', '龟兔 赛跑', '龟兹', '龟壳', '龟头', '龟山', '龟峰', '龟板', '龟田', '龟甲', '龟纹', '龟缩', '龟背', '龟背竹', '龟裂']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect_ngram.get_feature_names()[-100:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "fencoding=chardet.detect(b'\"\".join(tfidf_vect_ngram.get_feature_names()[0:100]')\n",
    "print(fencoding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "fencoding=chardet.detect(b'\"\".join(tfidf_vect_ngram.get_feature_names()[-100:-1]')\n",
    "print(fencoding) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试今日头条的200条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_feature_names()的长度： 1000000\n",
      "特征维度：1000000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.64      0.72        53\n",
      "          1       0.88      0.95      0.91       146\n",
      "\n",
      "avg / total       0.86      0.86      0.86       199\n",
      "\n",
      "历时： 360.63630270957947\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(min_df=2, max_df=1.0, \n",
    "                                   ngram_range=(1, 2), max_features=1000000) #  token_pattern='(?u)\\\\b\\\\w+\\\\b' 去掉单独的汉字\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "\n",
    "print(\"get_feature_names()的长度：\", len(tfidf_vect_ngram.get_feature_names()))\n",
    "\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "print(\"特征维度：%s\" % xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "# xtrain_count, xtest_count = tfIdf(2, 1.0, (1, 2))\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(xtrain_tfidf_ngram, train_y)\n",
    "\n",
    "y_prediction = svm.predict(xtest_tfidf_ngram)\n",
    "\n",
    "print(classification_report(y_true=test_y, y_pred=y_prediction))\n",
    "\n",
    "end = time.time()\n",
    "print(\"历时：\", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
      " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[20, 23, 26, 30, 41, 53, 55, 60, 63, 67, 72, 76, 77, 80, 82, 88, 89, 91, 96, 108, 114, 125, 159, 183, 184, 191, 199]\n"
     ]
    }
   ],
   "source": [
    "wrong_label_list = []\n",
    "for i, label_pair in enumerate(zip(test_y, y_prediction)):\n",
    "        if label_pair[0] != label_pair[1]:\n",
    "            wrong_label_list.append(i+1)\n",
    "print(len(wrong_label_list))\n",
    "print(wrong_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26, 30, 53, 60, 67, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：4220013\n",
      "预测的负样本个数 6433\n",
      "预测的正样本个数 6045\n",
      "预测结果中，正样本占比0.484453\n",
      "预测结果中，负样本占比0.515547\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "xtrain_count, xtest_count = tfIdf(2, 1.0, (1,2)) # 4, 0,5 (1,2) ; 2, 0.5 (1,2) ; 2, 1.0 (1,2); 1, 1.0 , (1,2);默认参数1, 1.0, (1, 1)\n",
    "# train_model_test(LinearSVC(), xtrain_count, train_y, xtest_count)\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "clf = CalibratedClassifierCV(svm) \n",
    "\n",
    "clf.fit(xtrain_count, train_y)\n",
    "\n",
    "y_proba = clf.predict_proba(xtest_count)\n",
    "\n",
    "# print(y_proba[0:100])\n",
    "\n",
    "lists = []\n",
    "for i,j in y_proba:  # 0.43-0.47 loss:0.212121\n",
    "    if i > 0.9:  # 0.5 16个\n",
    "        lists.append(0)\n",
    "    else:\n",
    "        lists.append(1)\n",
    "# print(lists)\n",
    "\n",
    "\n",
    "lable_total = 0\n",
    "label_positive = 0\n",
    "label_negative =0\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "for i, label in enumerate(lists):\n",
    "    if label == 0:\n",
    "        positive_list.append(i+1)\n",
    "        label_positive += 1\n",
    "        lable_total += 1\n",
    "    if label ==1:\n",
    "        negative_list.append(i+1)\n",
    "        label_negative += 1\n",
    "        lable_total += 1\n",
    "# print(\"预测的负样本为\", negative_list)\n",
    "print(\"预测的负样本个数\", len(negative_list))\n",
    "# print(\"预测的正样本为\", positive_list)\n",
    "print(\"预测的正样本个数\", len(positive_list))\n",
    "print(\"预测结果中，正样本占比%f\" % (label_positive / lable_total))\n",
    "print(\"预测结果中，负样本占比%f\" % (label_negative / lable_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8350"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1002 + 7348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6045\n"
     ]
    }
   ],
   "source": [
    "# negative_samples1 = negative_list\n",
    "\n",
    "positive_samples1 = positive_list\n",
    "\n",
    "print(len(positive_samples1))\n",
    "# print(len(negative_samples1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18092\n"
     ]
    }
   ],
   "source": [
    "print(len(open(\"./new_negative_sample_from_toutiao_42000_1.txt\", \"r\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12478\n"
     ]
    }
   ],
   "source": [
    "# # new_negative_samples = open(\"./new_negative_sample_from_toutiao_2000_tfidf_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "# new_negative_samples = open(\"./new_negative_sample_from_toutiao_100_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "# data = open(\"./JinRiTouTiao_format_2000.txt\", \"r\", encoding=\"utf-8\")\n",
    "# i = 1\n",
    "# for line in data:\n",
    "#     line = line.strip()\n",
    "#     if i in negative_samples1:\n",
    "#         new_negative_samples.write(\"1\" + \"\\t\" + line + \"\\n\")\n",
    "#         i += 1\n",
    "#     if i not in negative_samples1:\n",
    "#         i += 1\n",
    "#         continue\n",
    "# new_negative_samples.close()\n",
    "\n",
    "data = open(\"./JinRiTouTiao_format_92000_drop1.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "new_positive_samples = open(\"./new_positive_sample_from_toutiao_92000_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if i+1 in positive_samples1:\n",
    "        new_positive_samples .write(\"0\" + \"\\t\" + data[i])\n",
    "#     if i+1 in negative_samples1:\n",
    "#         new_negative_samples .write(\"1\" + \"\\t\" + data[i])\n",
    "        \n",
    "new_positive_samples.close()\n",
    "\n",
    "\n",
    "# new_negative_samples = open(\"./new_negative_sample_from_toutiao_92000_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# print(len(data))\n",
    "\n",
    "# for i in range(len(data)):\n",
    "\n",
    "#     if i+1 in negative_samples1:\n",
    "#         new_negative_samples .write(\"1\" + \"\\t\" + data[i])\n",
    "        \n",
    "\n",
    "# new_negative_samples.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本604个\n",
      "正样本600个，占比0.993377\n",
      "负样本4个，占比0.006623\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "统计预测结果修改后的正负样本数，统计预测结果\n",
    "'''\n",
    "\n",
    "with open(\"./new_positive_sample_from_toutiao_2000.txt\", \"r\", encoding=\"utf-8\") as result:\n",
    "    data = result.readlines()\n",
    "    po_label = 0\n",
    "    ne_label = 0\n",
    "    total_label = 0\n",
    "    \n",
    "    for d in data:\n",
    "        \n",
    "        label = d.split(\"\\t\")[0]\n",
    "        context = d.split(\"\\t\")[1]\n",
    "        total_label += 1\n",
    "        \n",
    "        if label == \"0\":\n",
    "            po_label += 1\n",
    "\n",
    "        if label == \"1\":\n",
    "            ne_label += 1\n",
    "\n",
    "print(\"总样本%d个\" % total_label)\n",
    "print(\"正样本%d个，占比%f\" % (po_label, (po_label / total_label)))\n",
    "print(\"负样本%d个，占比%f\" % (ne_label, (ne_label / total_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：899314\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "预测的负样本为 [532]\n",
      "预测的负样本个数 1\n",
      "预测结果中，正样本占比0.998366\n",
      "预测结果中，负样本占比0.001634\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "xtrain_count, xtest_count = countVector(2, 1.0, (1, 2))\n",
    "# train_model_test(LinearSVC(), xtrain_count, train_y, xtest_count)\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "clf = CalibratedClassifierCV(svm) \n",
    "\n",
    "clf.fit(xtrain_count, train_y)\n",
    "\n",
    "y_proba = clf.predict_proba(xtest_count)\n",
    "\n",
    "lists = []\n",
    "for i,j in y_proba:  # 0.43-0.47 loss:0.212121\n",
    "    if j > 0.9:  # 0.5 16个\n",
    "        lists.append(1)\n",
    "    else:\n",
    "        lists.append(0)\n",
    "print(lists)\n",
    "\n",
    "\n",
    "lable_total = 0\n",
    "label_positive = 0\n",
    "label_negative =0\n",
    "negative_list = []\n",
    "\n",
    "for i, label in enumerate(lists):\n",
    "    if label == 0:\n",
    "        label_positive += 1\n",
    "        lable_total += 1\n",
    "    if label ==1:\n",
    "        negative_list.append(i+1)\n",
    "        label_negative += 1\n",
    "        lable_total += 1\n",
    "print(\"预测的负样本为\", negative_list)\n",
    "print(\"预测的负样本个数\", len(negative_list))\n",
    "print(\"预测结果中，正样本占比%f\" % (label_positive / lable_total))\n",
    "print(\"预测结果中，负样本占比%f\" % (label_negative / lable_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：53694\n",
      "[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "0.21212121212121213\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.59      0.71        58\n",
      "          1       0.74      0.95      0.83        74\n",
      "\n",
      "avg / total       0.81      0.79      0.78       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "svm = LinearSVC()\n",
    "# clf = CalibratedClassifierCV(svm, cv=2, method='isotonic') \n",
    "# clf = CalibratedClassifierCV(svm, cv=2, method='sigmoid') \n",
    "clf = CalibratedClassifierCV(svm) \n",
    "xtrain_count, xvalid_count = countVector(1, 0.5, (1, 1))\n",
    "clf.fit(xtrain_count, train_y)\n",
    "y_proba = clf.predict_proba(xvalid_count)\n",
    "# print(y_proba)\n",
    "\n",
    "lists = []\n",
    "for i,j in y_proba:  # 0.43-0.47 loss:0.212121\n",
    "    if j > 0.56:\n",
    "        lists.append(1)\n",
    "    else:\n",
    "        lists.append(0)\n",
    "print(lists)\n",
    "\n",
    "print(brier_score_loss(valid_y, lists))\n",
    "print(classification_report(y_true=valid_y, y_pred=lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：15940\n",
      "pred label: [1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.63      0.69        54\n",
      "          1       0.77      0.87      0.82        78\n",
      "\n",
      "avg / total       0.77      0.77      0.77       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "SVM, N-Gram Vectors:  0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "#特征为多个词语级别TF-IDF向量的SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "xtrain_count, xvalid_count = tfIdf(3, 0.75, (1, 1))\n",
    "accuracy = train_model_valid(LinearSVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：372591\n",
      "pred label: [1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.70      0.69        54\n",
      "          1       0.79      0.77      0.78        78\n",
      "\n",
      "avg / total       0.74      0.74      0.74       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "RF, Count Vectors:  0.7424242424242424\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xtrain_count, xvalid_count = countVector(1, 0.75, (1, 2))\n",
    "accuracy = train_model_valid(RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"RF, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：22134\n",
      "pred label: [1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
      " 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0\n",
      " 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.80      0.75        54\n",
      "          1       0.85      0.77      0.81        78\n",
      "\n",
      "avg / total       0.79      0.78      0.78       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "RF, N-Gram Vectors:  0.7803030303030303\n"
     ]
    }
   ],
   "source": [
    "#特征为多个词语级别TF-IDF向量的RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xtrain_count, xvalid_count = tfIdf(2, 1.0, (1, 1))\n",
    "accuracy = train_model_valid(RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"RF, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：53694\n",
      "pred label: [1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.72      0.75        54\n",
      "          1       0.82      0.86      0.84        78\n",
      "\n",
      "avg / total       0.80      0.80      0.80       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "RF, Count Vectors:  0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lx/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的Xgboost\n",
    "import xgboost\n",
    "\n",
    "xtrain_count, xvalid_count = countVector(1, 0.5, (1, 1))\n",
    "accuracy = train_model_valid(xgboost.XGBClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"RF, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度：53714\n",
      "pred label: [1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
      " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.74      0.73        54\n",
      "          1       0.82      0.81      0.81        78\n",
      "\n",
      "avg / total       0.78      0.78      0.78       132\n",
      "\n",
      "true label: [1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1]\n",
      "RF, Count Vectors:  0.7803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lx/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#特征为多个词语级别TF-IDF向量的Xgboost\n",
    "import xgboost\n",
    "\n",
    "xtrain_count, xvalid_count = tfIdf(1, 0.75, (1, 1))\n",
    "accuracy = train_model_valid(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "print(\"true label:\", valid_y)\n",
    "print(\"RF, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
